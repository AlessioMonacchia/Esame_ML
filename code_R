#### Libraries ####
library(graphics)
library(terra)
library(pROC)
library(caret)
library(sf)
library(purrr)
library(tools)
library(tidyverse)
library(rsample)
library(furrr)
library(pbapply)
library(ranger)
library(tidymodels)
library(e1071)
library(ggridges)
library(gridExtra)
library(rlang)
library(rpart.plot)
# library(rgl)
library(scatterplot3d)
library(RVAideMemoire)
library(rstatix)
library(doMC)

#### Functions ####

source("R/clean.name.R") # scorciatoia per:

clean_name <- function(file) {
  
  var_name <- tools::file_path_sans_ext(basename(file))   # get file name without extension
  
  var_name <- gsub("[^[:alnum:]_]", "", var_name)
  
  if (grepl("^[0-9]", var_name)) {
    
    var_name <- paste0("x, var_name")
    
  }
  
  var_name
  
}


ridgeplot <- function(dataframe, x.variable, y.variable){
  
  ridge <- ggplot(dataframe, aes(x.variable, y.variable, fill = factor(stat(quantile)))) +
    
    stat_density_ridges(
      
      geom = "density_ridges_gradient", calc_ecdf = TRUE,
      
      quantiles = 4, quantile_lines = TRUE
      
    )
  
}


model_scores <- function(model_predictions, response_variable){
  
  matrice_confusione <- confusionMatrix(model_predictions, response_variable, positive = "fiore")
  
  accuracy <- matrice_confusione$overall['Accuracy']
  
  recall <- matrice_confusione$byClass["Sensitivity"]
  
  f1 <- matrice_confusione$byClass["F1"]
  
  auc_roc <- roc(test_data$label, as.numeric(model_predictions))
  
  auc <- auc(auc_roc)
  
  precision <- matrice_confusione$byClass["Pos Pred Value"]
  
  
  
  print(matrice_confusione)
  
  print(paste("Accuracy: ", accuracy))
  
  print(paste("Recall: ", recall))
  
  print(paste("F1-score: ", f1))
  
  print(paste("AUC-ROC", auc))
  
  print(paste("Precision: ", precision))
  
}

#### Import data ####

setwd("/home/alessio/Esame_ML")

# orthomosaics

ortho <- list.files(pattern = "Ortho", full.names = TRUE, recursive = TRUE)%>%
  
  set_names(nm = map(., clean_name)) %>%
  
  map(~rast(.))

ortho <- ortho[1]

# buffers

buffers <- list.files(pattern = "buffer.*shp",
                      
                      full.names = TRUE, recursive = TRUE)%>%
  
  set_names(nm = map(., clean_name))%>%
  
  map(~st_read(.))

buffers <- buffers[1]


# Fiori ed Erba

fiori <- list.files(pattern = "fiori.*shp", full.names = TRUE, recursive = TRUE)%>%
  
  set_names(nm =map(., clean_name)) %>%
  
  map(~st_read(.))%>%
  
  map(~.[!st_is_empty(.), ])

fiori <- fiori[1]


erba <- list.files(pattern = "erba.*shp", full.names = TRUE, recursive = TRUE)%>%
  
  set_names(nm = map(., clean_name)) %>%
  
  map(~st_read(.))%>%
  
  map(~.[!st_is_empty(.), ])                # linea di codice che esclude le geometrie vuote

erba <- erba[1]



#### Image pre-processing ####

# crop orthomosaics within buffer extentions

ortho <- map2(ortho, map(buffers, ~ ext(.x)), ~ crop(.x, .y))

# mask orthomosaics within buffer extentions

ortho <- map2(ortho, buffers, ~ mask(.x, .y))

# values extraction

values_fiori <- map2(ortho, fiori, ~terra::extract(.x, .y))

values_erba <- map2(ortho, erba, ~terra::extract(.x, .y))

# change column names for each element of the lists

df_fiori <- map(values_fiori, ~{
  
  colnames(.) <- c("ID_poly", paste0("banda_", 1:(ncol(.)-1)))
  
  .
  
})

df_erba <- map(values_erba, ~{
  
  colnames(.) <- c("ID_poly", paste0("banda_", 1:(ncol(.)-1)))
  
  .
  
})

# convert into dataframes

df_fiori_unified <- bind_rows(df_fiori, .id = "origin")   # .id = "origin": Questo argomento opzionale viene utilizzato per aggiungere 
# una colonna chiamata "origin" al risultato finale. 
# Questa colonna conterrÃ  il nome dell'oggetto (in questo caso, il dataframe) 
# da cui proviene ogni riga.

df_erba_unified <- bind_rows(df_erba, .id ="origin")

# add the label column to the data frames

df_fiori_unified$label <- "fiore"

df_erba_unified$label <- "erba"


#### Exploratory data analysis ####

# using ggridges for the density plots

df_tot <- rbind(df_fiori_unified, df_erba_unified) %>%
  
  rowid_to_column(.) %>% 
  
  rename(ID_pixel = rowid)


# Seleziono le bande

set.seed(123)


# Crea un nuovo dataframe con solo le colonne che vuoi usare

sum(is.na(df_tot))


df_selected <- df_tot %>%
  
  select(banda_1, banda_2, banda_3, label) %>%
  
  na.omit()


# calculate the main summary statistics for each cover type

subset_df <- df_selected %>% filter(df_selected$label == "fiore")

summary(subset_df)


# calcualte the interquantile range

IQR(subset_df$banda_1, na.rm = T)


# Ridgeplots

ridge1 <- ridgeplot(df_selected, df_selected$banda_1, df_selected$label)

ridge2 <- ridgeplot(df_selected, df_selected$banda_2, df_selected$label)

ridge3 <- ridgeplot(df_selected, df_selected$banda_3, df_selected$label)

grid.arrange(ridge1, ridge2, ridge3, ncol = 2)


# scatterplot: variables relation

pairs(df_fiori_unified[,3:5], pch = 19)

pairs(df_erba_unified[,3:5], pch = 19)


# Boxplot

par(mfrow = c(3,1))

ggplot(df_selected, aes(x = label, y = banda_1, fill = label)) +
  geom_boxplot() +
  labs(title = "Boxplot band 1", x = "Label", y = "Band_1") +
  theme_minimal()

ggplot(df_selected, aes(x = label, y = banda_2, fill = label)) +
  geom_boxplot() +
  labs(title = "Boxplot band 2", x = "Label", y = "Band_2") +
  theme_minimal()

ggplot(df_selected, aes(x = label, y = banda_3, fill = label)) +
  geom_boxplot() +
  labs(title = "Boxplot band 3", x = "Label", y = "Band_3") +
  theme_minimal()


# Provo il 3D plot

df_3D <- df_selected[c(1:100000), ]

# Add a new column with color
# mycolors <- c('royalblue1', 'oldlace')

# df_3D$color <- mycolors[ as.numeric(df_selected$label) ]

df_3D$color <- NA

df_3D_f <- which(df_3D$label == "fiore", )

df_3D_e <- which(df_3D$label == "erba", )

df_3D$color[df_3D_f] <- "red"

df_3D$color[df_3D_e] <- "green"

# Plot attempt with rgl package
# plot3d( x=df_3D$banda_1, y=df_3D$banda_2, z=df_3D$banda_3,
#   col = df_3D$color, 
#   size = 1,
#   type = 's',
#   radius = .1,
#   xlab="band 1", ylab="band 2", zlab="band 3")

# To display in an R Markdown document:
# rglwidget()

# To save to a file:
# htmlwidgets::saveWidget(rglwidget(width = 520, height = 520), 
#                         file = "HtmlWidget/3dscatter.html",
#                         libdir = "libs",
#                         selfcontained = FALSE
# )

# Attempt with scatterplot3D package

scatterplot3d(
  df_3D[,1:3], 
  pch = 16, 
  color = df_3D$color,
  #type = "h",
  grid=T, box=T)
legend("bottom", legend = c("flower", "grass"),
       col =  c("red", "green"), 
       pch = c(16), 
       inset = -0.25, xpd = T, horiz = T)



# Correlation

cor(df_selected[, 1:3])

# or

cor.test(df_selected[,2], df_selected[,3], method = "spearman")

#### Data split ####

# Dividi il dataframe selezionato in set di addestramento e di test

data_split <- initial_split(df_selected, prop = 0.7, strata = "label")

training_data <- training(data_split)

test_data <- testing(data_split)


summary(training_data)
summary(test_data)

# Convertiamo in un fattore

training_data$label <- as.factor(training_data$label)

test_data$label <- as.factor(test_data$label)


#### RF Model ####

# Define cross-validation strategy

train_control <- trainControl(method = "cv", number = 5,
                              
                              savePredictions = "final",
                              
                              verboseIter = TRUE)


# Train the model using ranger and the tuning grid

# Calcola i pesi delle classi

classi <- unique(training_data$label)

class_weights <- c("erba" = 0.1, "fiore" = 0.9)


# Define tuning parameters

RFGrid <- expand.grid(mtry = c(1, 2, 3),
                      
                      splitrule = "hellinger",
                      
                      min.node.size = c(1, 3, 5))


# Addestra il modello utilizzando i pesi delle classi

RF <- train(label ~ ., 
            
            data = training_data,
            
            method = "ranger",
            
            num.trees = 500,
            
            trControl = train_control,
            
            tuneGrid = RFGrid,
            
            class.weights = class_weights)


plot(RF)

print(RF)


#### K-nearest neighbor model ####

# Tuning parameters

KNNGrid <- expand.grid(k = c(3, 5, 10))


# Addestra il modello utilizzando i pesi delle classi

KNN <- train(label ~ ., 
             
             data = training_data,
             
             method = "knn",
             
             trControl = train_control,
             
             tuneGrid = KNNGrid)


plot(KNN)

#### Support Vector Machines model ####

# tuning parameters

SVMGrid <- expand.grid(cost = c(10, 20, 50))


# Addestra il modello utilizzando i pesi delle classi

SVM <- train(label ~ ., 
             
             data = training_data,
             
             method = "svmLinear2",
             
             trControl = train_control,
             
             tuneGrid = SVMGrid,
             
             class.weights = class_weights)


plot(SVM)

#### NNE Model ####

# comprehensible alternative

NNE_grid <- expand.grid(decay = c(0.5, 1e-2, 1e-4), # weight decay
                        
                        size = c(2, 4, 8)) # number of hidden units


# Train the neural network using train()

NNE <- train(label ~ ., 
             
             data = training_data, 
             
             method = "nnet",
             
             trControl = train_control,
             
             tuneGrid = NNE_grid, 
             
             class.weights = class_weights,
             
             allowParallel = T)


# Print the trained model

plot(NNE)

print(NNE)

### Model Assessment ####

predizioni_RF <- predict(RF, test_data)

predizioni_KNN <- predict(KNN, test_data)

predizioni_SVM <- predict(SVM, test_data)

predizioni_NNE <- predict(NNE, test_data)


scores_RF <- model_scores(predizioni_RF, test_data$label)

scores_kNN <- model_scores(predizioni_KNN, test_data$label)

scores_SVM <- model_scores(predizioni_SVM, test_data$label)

scores_NNE <- model_scores(predizioni_NNE, test_data$label)


# PLots

df <- cbind(test_data, predizioni = predizioni_RF)

ggplot(training_data, aes(banda_2, banda_3, fill = label)) +
  geom_raster(data = df, alpha = 0.5) +
  geom_point(shape = 21, size = 3) +
  theme_minimal()


#### Model Comparison ####

# For multiple comparisons
# Prapare the data for the Cochran's Q test

model1 <- data.frame(
  outcome = ifelse(as.vector(predizioni_RF) == as.vector(test_data$label), 1, 0),
  algorithm = gl(1, length(predizioni_RF), length(predizioni_RF), labels = "RF"),
  ID = 1:length(predizioni_RF))
#  sample_frac(0.1)

model2 <- data.frame(
  outcome = ifelse(as.vector(predizioni_KNN) == as.vector(test_data$label), 1, 0),
  algorithm = gl(1, length(predizioni_KNN), length(predizioni_KNN), labels = "KNN"),
  ID = 1:length(predizioni_KNN))
# %>% sample_frac(0.1)

model3 <- data.frame(
  outcome = ifelse(as.vector(predizioni_SVM) == as.vector(test_data$label), 1, 0),
  algorithm = gl(1, length(predizioni_SVM), length(predizioni_SVM), labels = "SVM"),
  ID = 1:length(predizioni_SVM)) 
# %>% sample_frac(0.1)

model4 <- data.frame(
  outcome = ifelse(as.vector(predizioni_NNE) == as.vector(test_data$label), 1, 0),
  algorithm = gl(1, length(predizioni_NNE), length(predizioni_NNE), labels = "NNE"),
  ID = 1:length(predizioni_NNE))
# %>% sample_frac(0.1)


# Tentativo con solo fiori

# RF 

fiori_RF <- which(predizioni_RF == "fiore")  # indexing

fiori_test_RF <- test_data[fiori_RF, ]

fiori_RF <- as.vector(predizioni_RF[predizioni_RF == "fiore"])


model1_f <- data.frame(
  outcome = ifelse(fiori_RF == as.vector(fiori_test_RF$label), 1, 0),
  algorithm = gl(1, length(fiori_RF), length(fiori_RF), labels = "RF"),
  ID = 1:length(fiori_RF))

# KNN

fiori_KNN <- which(predizioni_KNN == "fiore")  # indexing

fiori_test_KNN <- test_data[fiori_KNN, ]

fiori_KNN <- as.vector(predizioni_KNN[predizioni_KNN == "fiore"])


model2_f <- data.frame(
  outcome = ifelse(fiori_KNN == as.vector(fiori_test_KNN$label), 1, 0),
  algorithm = gl(1, length(fiori_KNN), length(fiori_KNN), labels = "KNN"),
  ID = 1:length(fiori_KNN))

# SVM

fiori_SVM <- which(predizioni_SVM == "fiore")  # indexing

fiori_test_SVM <- test_data[fiori_SVM, ]

fiori_SVM <- as.vector(predizioni_SVM[predizioni_SVM == "fiore"])


model3_f <- data.frame(
  outcome = ifelse(fiori_SVM == as.vector(fiori_test_SVM$label), 1, 0),
  algorithm = gl(1, length(fiori_SVM), length(fiori_SVM), labels = "SVM"),
  ID = 1:length(fiori_SVM))

# NNE

fiori_NNE <- which(predizioni_NNE == "fiore")  # indexing

fiori_test_NNE <- test_data[fiori_NNE, ]

fiori_NNE <- as.vector(predizioni_NNE[predizioni_NNE == "fiore"])


model4_f <- data.frame(
  outcome = ifelse(fiori_NNE == as.vector(fiori_test_NNE$label), 1, 0),
  algorithm = gl(1, length(fiori_NNE), length(fiori_NNE), labels = "NNE"),
  ID = 1:length(fiori_NNE))


# Bind in a single dataframe

Test <- rbind(model1, model2, model3, model4)

Test_f <- rbind(model1_f, model2_f, model3_f, model4_f)

# adjust ID values 

# Test$ID[Test$algorithm == "RF"] <- 1:sum(Test$algorithm == "RF")
# Test$ID[Test$algorithm == "KNN"] <- 1:sum(Test$algorithm == "KNN")
# Test$ID[Test$algorithm == "SVM"] <- 1:sum(Test$algorithm == "SVM")
# Test$ID[Test$algorithm == "NNE"] <- 1:sum(Test$algorithm == "NNE")


# convert the outcome into a factor with 2 levels (not for Test_filtered)

Test_f$outcome <- factor(
  
  Test_f$outcome, levels = c(1, 0),
  
  labels = c("success", "failure")
  
)


# Cross-tabulation: create a contingency table from cross-classifying factors

xtabs(~outcome + algorithm, Test_f)


# Then apply the Cochran's Q test

cochran.qtest(outcome ~ algorithm|ID, Test_f)


# Then apply the pairwise McNemar test

pairwise_mcnemar_test(Test_f, outcome ~ algorithm|ID)

# OR

# pairwise_mcnemar_test(data,
#  formula,
#  type = c("mcnemar", "exact"),
#  correct = TRUE,
#  p.adjust.method = "bonferroni"
#)

#### Raster creation ####

# aggiustiamo i nomi

ortho$Ortho_BE1$`Ortho_BE1_4` <- NULL

ortho <- map(ortho, function(x) {
  
  names(x) <- colnames(df_selected)[1:3]
  
  return(x)
  
})



# Creare una lista vuota per memorizzare i risultati

classificazione_KNN_list <- list()

# Ciclo for per applicare predizioni modelli a tutti gli elementi di ortho

for (name in names(ortho)) {
  
  previsioni <- terra::predict(ortho[[name]], KNN, na.rm = TRUE)
  
  classificazione_KNN_list[[name]] <- previsioni
  
}


# Salvare la classificazione in formato GeoTiff

names(classificazione_KNN_list) <- names(ortho) # Assicuriamoci che i nomi corrispondano


walk(names(classificazione_KNN_list), function(name) {
  
  terra::writeRaster(classificazione_KNN_list[[name]],
                     
                     filename = paste0(name, "KNN.tif"),
                     
                     overwrite = TRUE)
  
})



# Per NNe

classificazione_SVM_list <- list()

# Ciclo for per applicare predizioni modelli a tutti gli elementi di ortho

for (name in names(ortho)) {
  
  previsioni <- terra::predict(ortho[[name]], SVM, na.rm = TRUE)
  
  classificazione_SVM_list[[name]] <- previsioni
  
}


# Salvare la classificazione in formato GeoTiff

names(classificazione_SVM_list) <- names(ortho) # Assicuriamoci che i nomi corrispondano


walk(names(classificazione_SVM_list), function(name) {
  
  terra::writeRaster(classificazione_SVM_list[[name]],
                     
                     filename = paste0(name, "SVM.tif"),
                     
                     overwrite = TRUE)
  
})
